{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pylts Docs","text":"<p>A thin, reusable pydantic wrapper over litestream applied specifically to an s3 bucket.</p> <p>This makes it easier to use litestream inside a python script to handle the replicate and restore aspects of this flow:</p> <pre><code>---\ntitle: From Local to Bucket to Production\n---\nflowchart LR\n  subgraph fly.io\n    volume\n    app\n  end\n  subgraph aws\n    s3(s3 bucket)--pylts restore volume--&gt;app\n  end\n  subgraph local\n    db--\"litestream replicate (args)\"--&gt;s3\n    code--fly deploy--&gt;app\n  end</code></pre> Environment Task local Set up repo with a sqlite.db (<code>db</code>) local With <code>pydantic</code>-parsed values, <code>litestream replicate</code> this created <code>db</code> to <code>s3</code> local Deploy the repo to <code>fly.io</code> prod Pydantically restore <code>db</code> from <code>s3</code> to <code>fly.io</code> <p>The database that is (re)stored in the volume (fly.io) may be updated either by cron jobs or by adding, deleting entries directly. How do we ensure these updates are persisted to the s3 bucket?</p> <pre><code>---\ntitle: From Production to Bucket to Local\n---\nflowchart LR\n  subgraph fly.io\n    volume\n    app\n  end\n  subgraph aws\n    app--pylts replicate volume--&gt;s3\n  end\n  subgraph local\n    s3--\"litestream restore (args)\"--&gt;db\n    code\n  end</code></pre>"},{"location":"#steps","title":"Steps","text":"<ol> <li>Add fields from s3 bucket to <code>.env</code></li> <li><code>.env</code> picked up by Pydantic BaseSettings model</li> <li>Python subprocesses to litestream replicate/restore use BaseSettings</li> </ol>"},{"location":"#configuration","title":"Configuration","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Generate a configuration instance to determine how to access the replica url and where to place the local db from such url.</p>"},{"location":"#pylts.aws.ConfigS3--replica-url","title":"Replica url","text":"<p>Follow instructions to get:</p> Field Type Description Declare in .env key str Access Key <code>LITESTREAM_ACCESS_KEY_ID</code> token str Secret Token <code>LITESTREAM_SECRET_ACCESS_KEY</code> s3 str e.g. <code>s3://&lt;bucket_name&gt;/&gt;/&lt;folder&gt;</code> <code>REPLICA_URL</code>"},{"location":"#pylts.aws.ConfigS3--local-db","title":"Local db","text":"<p>There are two fields that can be declared affecting the local database path</p> Field Type Description <code>db</code> str Optional, Default: db.sqlite. Must end in .db or .sqlite <code>folder</code> pathlib.Path Default: root folder based on Path(file). Where to place the <code>db</code> <p>The <code>@dbpath</code> is based on <code>folder</code> / <code>db</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; stream\nConfigS3(s3='s3://x/x.db', db='db.sqlite')\n&gt;&gt;&gt; stream.dbpath.name\n'db.sqlite'\n&gt;&gt;&gt; stream.dbpath.parent.stem\n'data'\n&gt;&gt;&gt; stream.dbpath.parent.parent.stem\n'pylts'\n</code></pre> Source code in <code>pylts/aws.py</code> Python<pre><code>class ConfigS3(BaseSettings):\n\"\"\"Generate a configuration instance to determine how to access the _replica url_ and\n    where to place the _local db_ from such url.\n    ## Replica url\n    Follow [instructions](https://litestream.io/guides/s3/) to get:\n    Field | Type | Description | Declare in .env\n    --:|:--:|:--|:--\n    key | str | Access Key | `LITESTREAM_ACCESS_KEY_ID`\n    token | str | Secret Token | `LITESTREAM_SECRET_ACCESS_KEY`\n    s3 | str | e.g. `s3://&lt;bucket_name&gt;/&gt;/&lt;folder&gt;` | `REPLICA_URL`\n    ## Local db\n    There are two fields that can be declared affecting the local database path\n    Field | Type | Description\n    --:|:--:|:-\n    `db` | str | Optional, Default: _db.sqlite_. Must end in .db or .sqlite\n    `folder` | pathlib.Path | Default: root folder based on Path(__file__). Where to place the `db`\n    The `@dbpath` is based on `folder` / `db`.\n    Examples:\n        &gt;&gt;&gt; from pylts import ConfigS3\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n        &gt;&gt;&gt; stream\n        ConfigS3(s3='s3://x/x.db', db='db.sqlite')\n        &gt;&gt;&gt; stream.dbpath.name\n        'db.sqlite'\n        &gt;&gt;&gt; stream.dbpath.parent.stem\n        'data'\n        &gt;&gt;&gt; stream.dbpath.parent.parent.stem\n        'pylts'\n    \"\"\"  # noqa: E501\nkey: str = Field(\ndefault=...,\nrepr=False,\ntitle=\"AWS Access Key ID\",\nenv=\"LITESTREAM_ACCESS_KEY_ID\",\n)\ntoken: str = Field(\ndefault=...,\nrepr=False,\ntitle=\"AWS Secret Key\",\nenv=\"LITESTREAM_SECRET_ACCESS_KEY\",\n)\ns3: str = Field(\ndefault=...,\nrepr=True,\ntitle=\"s3 URL\",\ndescription=\"Should be in the format: s3://bucket/pathname\",\nenv=\"REPLICA_URL\",\nregex=r\"^s3:\\/\\/.*$\",\nmax_length=100,\n)\nfolder: Path = Field(\ndefault=Path(__file__).parent.parent / \"data\",\nrepr=False,\ntitle=\"Folder\",\ndescription=\"Should be folder in the root's /data path\",\n)\ndb: str = Field(\ndefault=\"db.sqlite\",\ntitle=\"Database File\",\ndescription=\"Where db will reside in client.\",\nenv=\"DB_SQLITE\",\nregex=r\"^[a-z]{1,20}.*\\.(sqlite|db)$\",\nmax_length=50,\n)\nclass Config:\nenv_file = \".env\"\nenv_file_encoding = \"utf-8\"\n@property\ndef dbpath(self) -&gt; Path:\nself.folder.mkdir(exist_ok=True)\nreturn self.folder / self.db\n@property\ndef replicate_args(self):\nreturn [\n\"litestream\",\n\"replicate\",\nstr(self.dbpath),  # path to loca\nself.s3,  # where to replicate\n]\ndef replicate(self) -&gt; Path:\nreturn self.run(self.replicate_args)\n@property\ndef restore_args(self):\nreturn [\n\"litestream\",\n\"restore\",\n\"-v\",  # verbose\n\"-o\",  # output\nf\"{str(self.dbpath)}\",  # output path\nself.s3,  # source of restore\n]\ndef restore(self) -&gt; Path:\nreturn self.run(run_args=self.restore_args)\ndef delete(self):\nlogger.warning(f\"Deleting {self.dbpath=}\")\nself.dbpath.unlink(missing_ok=True)\ndef run(self, run_args: list[str]) -&gt; Path:\ncmd = {\" \".join(run_args)}\nlogger.info(f\"Run: {cmd}\")\nlogger.debug(subprocess.run(run_args, capture_output=True))\nreturn self.dbpath\ndef output(self, cmd: list[str], timeout: int) -&gt; tuple[str, str]:\np = Popen(cmd, text=True, stdout=PIPE, stderr=PIPE)\ntry:\nlogger.info(f\"Output: {cmd=}\")\nreturn p.communicate(timeout=timeout)\nexcept TimeoutExpired:\nlogger.info(f\"Timed Out: {cmd=}\")\np.kill()\nreturn p.communicate()\ndef timed_replicate(self, timeout_seconds: int) -&gt; bool:\n\"\"\"The replication process should be completed\n        within `timeout_seconds`; if successful, a snapshot\n        is written to the `s3` url and the local `@dbpath`\n        is deleted.\n        Args:\n            timeout_seconds (int): Number of seconds\n        Returns:\n            bool: Whether the replication was successfulw within `timeout_seconds`\n        \"\"\"\nlogger.info(\nf\"Replication to {self.s3=} start: {datetime.datetime.now()=}\"\n)\nres = self.output(self.replicate_args, timeout_seconds)\n_, stderr_data = res[0], res[1]\nfor text in stderr_data.splitlines():\nif \"snapshot written\" in text:\nlogger.success(f\"Snapshot on {datetime.datetime.now()=}\")\nself.dbpath.unlink()\nreturn True\nreturn False\n</code></pre>"},{"location":"#pylts.aws.ConfigS3-functions","title":"Functions","text":""},{"location":"#pylts.aws.ConfigS3.timed_replicate","title":"<code>timed_replicate(timeout_seconds)</code>","text":"<p>The replication process should be completed within <code>timeout_seconds</code>; if successful, a snapshot is written to the <code>s3</code> url and the local <code>@dbpath</code> is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_seconds</code> <code>int</code> <p>Number of seconds</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the replication was successfulw within <code>timeout_seconds</code></p> Source code in <code>pylts/aws.py</code> Python<pre><code>def timed_replicate(self, timeout_seconds: int) -&gt; bool:\n\"\"\"The replication process should be completed\n    within `timeout_seconds`; if successful, a snapshot\n    is written to the `s3` url and the local `@dbpath`\n    is deleted.\n    Args:\n        timeout_seconds (int): Number of seconds\n    Returns:\n        bool: Whether the replication was successfulw within `timeout_seconds`\n    \"\"\"\nlogger.info(\nf\"Replication to {self.s3=} start: {datetime.datetime.now()=}\"\n)\nres = self.output(self.replicate_args, timeout_seconds)\n_, stderr_data = res[0], res[1]\nfor text in stderr_data.splitlines():\nif \"snapshot written\" in text:\nlogger.success(f\"Snapshot on {datetime.datetime.now()=}\")\nself.dbpath.unlink()\nreturn True\nreturn False\n</code></pre>"},{"location":"api/","title":"Litestream AWS API","text":"<p>This requires an instantiated configuration:</p> Python<pre><code>from pylts import ConfigS3\ntry:\nstream = ConfigS3() # uses .env and default folder/db\nexcept ValidationError as e:\nraise Exception(f\"Missing fields; see {e=}\")\n</code></pre>"},{"location":"api/#restore","title":"Restore","text":"<p>Download the database in <code>@dbpath</code> from the replica url <code>s3://</code></p> Python<pre><code>&gt;&gt;&gt; stream.restore() # equivalent to litestream restore ...\n</code></pre>"},{"location":"api/#delete","title":"Delete","text":"<p>Delete the database in the folder specified.</p> Python<pre><code>&gt;&gt;&gt; stream.delete() # used prior to restoration\n</code></pre>"},{"location":"api/#replicate","title":"Replicate","text":"<p>Upload the database in <code>@dbpath</code> to replica url <code>s3://</code></p> Python<pre><code>&gt;&gt;&gt; stream.replicate() # equivalent to litestream replicate ...\n</code></pre>"},{"location":"cli/","title":"Sample CLI","text":"<p>See sample setup to generate <code>click</code>-based commands based on the secrets included in the .env file.</p>"},{"location":"cli/#pylts.cli-classes","title":"Classes","text":""},{"location":"cli/#pylts.cli-functions","title":"Functions","text":""},{"location":"cli/#pylts.cli.aws_replicate_db","title":"<code>aws_replicate_db()</code>","text":"<p>Wrapper around litestream to create a copy of the database to a preconfigured bucket in AWS. This assumes secrets have been previously set. Will now be usable as a CLI via <code>python -m pylts aws_replicate_db</code> for this app.</p> Source code in <code>pylts/cli.py</code> Python<pre><code>@click.command()\ndef aws_replicate_db():\n\"\"\"Wrapper around litestream to create a copy of the database to a\n    preconfigured bucket in AWS. This assumes secrets have been previously set.\n    Will now be usable as a CLI via `python -m pylts aws_replicate_db` for this app.\n    \"\"\"\nlitestream.replicate()\n</code></pre>"},{"location":"cli/#pylts.cli.aws_restore_db","title":"<code>aws_restore_db()</code>","text":"<p>Wrapper around litestream to download a copy of the database from a preconfigured bucket in AWS. This assumes secrets have been previously set. Will now be usable as a CLI via <code>python -m pylts aws_restore_db</code> for this app.</p> Source code in <code>pylts/cli.py</code> Python<pre><code>@click.command()\ndef aws_restore_db():\n\"\"\"Wrapper around litestream to download a copy of the database from a\n    preconfigured bucket in AWS. This assumes secrets have been previously set.\n    Will now be usable as a CLI via `python -m pylts aws_restore_db` for this app.\n    \"\"\"\nlitestream.delete()\nlitestream.restore()\n</code></pre>"},{"location":"docker/","title":"Dockerfile Sample","text":"<p>The steps included in the Dockerfile can be used as base example:</p> Elements Version Version List python <code>3.11.1</code> versions litestream <code>0.3.9</code> versions sqlite <code>3.40.1</code> versions"},{"location":"docker/#base","title":"Base","text":"<p>Get preliminary tools to process litestream and sqlite</p> Docker<pre><code>RUN apt update &amp;&amp; apt install -y build-essential wget pkg-config git &amp;&amp; apt clean\n</code></pre>"},{"location":"docker/#litestream","title":"Litestream","text":"Docker<pre><code># As of Feb 2023: latest version\nARG LITESTREAM_VER=0.3.9\nADD https://github.com/benbjohnson/litestream/releases/download/v$LITESTREAM_VER/litestream-v$LITESTREAM_VER-linux-amd64-static.tar.gz /tmp/litestream.tar.gz\nRUN tar -C /usr/local/bin -xzf /tmp/litestream.tar.gz\n</code></pre> <p>Review the latest version to get the updated release from github.</p>"},{"location":"docker/#sqlite","title":"sqlite","text":"<p>The version that comes with python isn't the most updated sqlite version, hence need to compile the latest one and configure with extensions:</p> Docker<pre><code># As of Feb 2023: latest version\nARG SQLITE_YEAR=2022\nARG SQLITE_VER=3400100\nRUN wget \"https://www.sqlite.org/$SQLITE_YEAR/sqlite-autoconf-$SQLITE_VER.tar.gz\" \\\n&amp;&amp; tar xzf sqlite-autoconf-$SQLITE_VER.tar.gz \\\n&amp;&amp; cd sqlite-autoconf-$SQLITE_VER \\\n&amp;&amp; ./configure --disable-static --enable-fts5 --enable-json1 CFLAGS=\"-g -O2 -DSQLITE_ENABLE_JSON1\" \\\n&amp;&amp; make &amp;&amp; make install\n</code></pre>"},{"location":"docker/#build-run-sample","title":"Build / Run Sample","text":"Bash<pre><code>export LITESTREAM_ACCESS_KEY_ID=xxx\nexport LITESTREAM_SECRET_ACCESS_KEY=yyy\nexport REPLICA_URL=s3://x/x.db\npoetry export -f requirements.txt -o requirements.txt --without-hashes\ndocker build .\ndocker run it \\\n-e LITESTREAM_ACCESS_KEY_ID \\\n-e LITESTREAM_SECRET_ACCESS_KEY \\\n-e REPLICA_URL\n</code></pre>"}]}