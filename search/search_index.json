{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pylts Docs","text":"<p>A thin, reusable pydantic wrapper over limited litestream functions: makes it easier to use litestream inside a python script to handle the replicate and restore aspects of an sqlite database in an s3 bucket.</p>"},{"location":"#usage","title":"Usage","text":"Python<pre><code># assumes installation via pip install pylts / poetry add pylts\n&gt;&gt;&gt; from pylts import ConfigS3 # inherits from pydantic.BaseSettings\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; d = Path().cwd() / \"data\" # where to place database\n&gt;&gt;&gt; stream = ConfigS3(folder=d) # assumes .env variables based on &lt;s3&gt; bucket\n&gt;&gt;&gt; stream.dbpath # constructed local &lt;db&gt; path based on folder\n&gt;&gt;&gt; stream.s3 # replica s3:// url in aws &lt;s3&gt;\n&gt;&gt;&gt; stream.restore() # subprocess: 'litestream restore -o &lt;db&gt; &lt;s3&gt;'\n&gt;&gt;&gt; stream.timed_replicate() # subprocess: 'litestream replicate &lt;db&gt; &lt;s3&gt;'\n</code></pre>"},{"location":"#flow","title":"Flow","text":""},{"location":"#placement","title":"Placement","text":"<pre><code>---\ntitle: From Local to Bucket to Production\n---\nflowchart LR\n  subgraph fly.io\n    volume\n    app\n  end\n  subgraph aws\n    s3(s3 bucket)--pylts restore volume--&gt;app\n  end\n  subgraph local\n    db--\"litestream replicate (args)\"--&gt;s3\n    code--fly deploy--&gt;app\n  end</code></pre> Environment Task local Set up repo with a sqlite.db (<code>db</code>) local With <code>pydantic</code>-parsed values, <code>litestream replicate</code> this created <code>db</code> to <code>s3</code> local Deploy the repo to <code>fly.io</code> prod Pydantically restore <code>db</code> from <code>s3</code> to <code>fly.io</code>"},{"location":"#retrieval","title":"Retrieval","text":"<p>The database that is (re)stored in the volume (fly.io) may be updated either by cron jobs or by adding, deleting entries directly. How do we ensure these updates are persisted to the s3 bucket?</p> <pre><code>---\ntitle: From Production to Bucket to Local\n---\nflowchart LR\n  subgraph fly.io\n    volume\n    app\n  end\n  subgraph aws\n    app--pylts replicate volume--&gt;s3\n  end\n  subgraph local\n    s3--\"litestream restore (args)\"--&gt;db\n    code\n  end</code></pre>"},{"location":"config/","title":"Configuration","text":"<p>         Bases: <code>BaseSettings</code></p>"},{"location":"config/#pylts.aws.ConfigS3--configs3","title":"ConfigS3","text":""},{"location":"config/#pylts.aws.ConfigS3--flow","title":"Flow","text":"<ol> <li>Add fields from s3 bucket to <code>.env</code></li> <li><code>.env</code> picked up by Pydantic <code>BaseSettings</code> model <code>ConfigS3</code></li> <li>Python subprocesses to litestream replicate/restore will based on <code>ConfigS3</code></li> </ol>"},{"location":"config/#pylts.aws.ConfigS3--env","title":".env","text":"<p>The necessary fields to declare in <code>.env</code> are:</p> <ul> <li><code>LITESTREAM_ACCESS_KEY_ID</code></li> <li><code>LITESTREAM_SECRET_ACCESS_KEY</code></li> <li><code>REPLICA_URL</code></li> </ul>"},{"location":"config/#pylts.aws.ConfigS3--replica-url","title":"Replica URL","text":"<p>The replica url is the s3 bucket that will host the sqlite database, i.e. the db will be replicated here or the db will be restored from here.</p> <p>For AWS, the replica url is formatted likeso: <code>s3://&lt;bucket_name&gt;/&lt;path&gt;</code>.</p> <p>To create the bucket, follow litestream instructions. This results in some values that we can use to create the <code>ConfigS3</code> instance:</p> Field Type Description Declare in .env key str Access Key <code>LITESTREAM_ACCESS_KEY_ID</code> token str Secret Token <code>LITESTREAM_SECRET_ACCESS_KEY</code> s3 str Bucket URL <code>REPLICA_URL</code> <p>Note that these values can/should be declared in the <code>.env</code> file:</p> Bash<pre><code># contents of .env\nLITESTREAM_ACCESS_KEY_ID=xxx\nLITESTREAM_SECRET_ACCESS_KEY=yyy\nREPLICA_URL=s3://zzz/db\n</code></pre> <p>The configuration ensures that we can create a subprocess that is authorized to perform actions with the replica url.</p>"},{"location":"config/#pylts.aws.ConfigS3--local-db","title":"Local db","text":"<p>The subprocesses involve a path to the database.</p> <p>There are two fields that can be declared in the configuration that can affect this path:</p> Field Type Description <code>db</code> str Optional, Default: db.sqlite. Must end in .db or .sqlite <code>folder</code> pathlib.Path Default: root folder based on Path(file). Where to place the <code>db</code> <p>The <code>@dbpath</code> is based on <code>folder</code> / <code>db</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; stream\nConfigS3(s3='s3://x/x.db', db='db.sqlite')\n&gt;&gt;&gt; stream.dbpath.name\n'db.sqlite'\n&gt;&gt;&gt; stream.dbpath.parent.stem\n'data'\n&gt;&gt;&gt; stream.dbpath.parent.parent.stem\n'pylts'\n</code></pre> Source code in <code>pylts/aws.py</code> Python<pre><code>class ConfigS3(BaseSettings):\n\"\"\"\n    # ConfigS3\n    ## Flow\n    1. Add fields from s3 bucket to `.env`\n    2. `.env` picked up by Pydantic `BaseSettings` model `ConfigS3`\n    3. Python subprocesses to litestream replicate/restore will based on `ConfigS3`\n    ## .env\n    The necessary fields to declare in `.env` are:\n    - `LITESTREAM_ACCESS_KEY_ID`\n    - `LITESTREAM_SECRET_ACCESS_KEY`\n    - `REPLICA_URL`\n    ## Replica URL\n    The _replica url_ is the s3 bucket that will host the sqlite database,\n    i.e. the db will be replicated here or the db will be restored from here.\n    For AWS, the replica url is formatted likeso: `s3://&lt;bucket_name&gt;/&lt;path&gt;`.\n    To create the bucket, follow [litestream instructions](https://litestream.io/guides/s3/).\n    This results in some values that we can use to create the `ConfigS3` instance:\n    Field | Type | Description | Declare in .env\n    --:|:--:|:--|:--\n    key | str | Access Key | `LITESTREAM_ACCESS_KEY_ID`\n    token | str | Secret Token | `LITESTREAM_SECRET_ACCESS_KEY`\n    s3 | str | Bucket URL | `REPLICA_URL`\n    Note that these values can/should be declared in the `.env` file:\n    ```sh\n    # contents of .env\n    LITESTREAM_ACCESS_KEY_ID=xxx\n    LITESTREAM_SECRET_ACCESS_KEY=yyy\n    REPLICA_URL=s3://zzz/db\n    ```\n    The configuration ensures that we can create a subprocess that is authorized\n    to perform actions with the _replica url_.\n    ## Local db\n    The subprocesses involve a path to the database.\n    There are two fields that can be declared in the configuration that can affect\n    this path:\n    Field | Type | Description\n    --:|:--:|:-\n    `db` | str | Optional, Default: _db.sqlite_. Must end in .db or .sqlite\n    `folder` | pathlib.Path | Default: root folder based on Path(__file__). Where to place the `db`\n    The `@dbpath` is based on `folder` / `db`.\n    Examples:\n        &gt;&gt;&gt; from pylts import ConfigS3\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n        &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n        &gt;&gt;&gt; stream\n        ConfigS3(s3='s3://x/x.db', db='db.sqlite')\n        &gt;&gt;&gt; stream.dbpath.name\n        'db.sqlite'\n        &gt;&gt;&gt; stream.dbpath.parent.stem\n        'data'\n        &gt;&gt;&gt; stream.dbpath.parent.parent.stem\n        'pylts'\n    \"\"\"  # noqa: E501\nkey: str = Field(\ndefault=...,\nrepr=False,\ntitle=\"AWS Access Key ID\",\nenv=\"LITESTREAM_ACCESS_KEY_ID\",\n)\ntoken: str = Field(\ndefault=...,\nrepr=False,\ntitle=\"AWS Secret Key\",\nenv=\"LITESTREAM_SECRET_ACCESS_KEY\",\n)\ns3: str = Field(\ndefault=...,\nrepr=True,\ntitle=\"s3 URL\",\ndescription=\"Should be in the format: s3://bucket/pathname\",\nenv=\"REPLICA_URL\",\nregex=r\"^s3:\\/\\/.*$\",\nmax_length=100,\n)\nfolder: Path = Field(\ndefault=Path(__file__).parent.parent / \"data\",\nrepr=False,\ntitle=\"Folder\",\ndescription=\"Should be folder in the root's /data path\",\n)\ndb: str = Field(\ndefault=\"db.sqlite\",\ntitle=\"Database File\",\ndescription=\"Where db will reside in client.\",\nenv=\"DB_SQLITE\",\nregex=r\"^[a-z]{1,20}.*\\.(sqlite|db)$\",\nmax_length=50,\n)\nclass Config:\nenv_file = \".env\"\nenv_file_encoding = \"utf-8\"\n@property\ndef dbpath(self) -&gt; Path:\n\"\"\"Examples:\n            &gt;&gt;&gt; from pylts import ConfigS3\n            &gt;&gt;&gt; from pathlib import Path\n            &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n            &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n            &gt;&gt;&gt; stream.dbpath == Path().cwd() / \"data\" / \"db.sqlite\" # Automatic construction of default db name\n            True\n        Returns:\n            Path: Where the database will be located locally.\n        \"\"\"  # noqa: E501\nself.folder.mkdir(exist_ok=True)\nreturn self.folder / self.db\n@property\ndef replicate_args(self) -&gt; list[str]:\n\"\"\"When used in the command line `litestream replicate &lt;dbpath&gt; &lt;replica_url&gt;`\n        works. As a subprocess, we itemize each item for future use.\n        Examples:\n            &gt;&gt;&gt; from pylts import ConfigS3\n            &gt;&gt;&gt; from pathlib import Path\n            &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n            &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n            &gt;&gt;&gt; args = stream.replicate_args\n            &gt;&gt;&gt; isinstance(stream.replicate_args, list)\n            True\n            &gt;&gt;&gt; args[0] == \"litestream\"\n            True\n            &gt;&gt;&gt; args[1] == \"replicate\"\n            True\n            &gt;&gt;&gt; args[2] == str(stream.dbpath)\n            True\n            &gt;&gt;&gt; args[3] == stream.s3\n            True\n        \"\"\"  # noqa: E501\nreturn [\n\"litestream\",\n\"replicate\",\nstr(self.dbpath),  # path to loca\nself.s3,  # where to replicate\n]\n@property\ndef restore_args(self) -&gt; list[str]:\n\"\"\"When used in the command line `litestream restore -o &lt;dbpath&gt; &lt;replica_url&gt;`\n        works. As a subprocess, we itemize each item for future use.\n        Examples:\n            &gt;&gt;&gt; from pylts import ConfigS3\n            &gt;&gt;&gt; from pathlib import Path\n            &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n            &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n            &gt;&gt;&gt; args = stream.restore_args\n            &gt;&gt;&gt; isinstance(stream.restore_args, list)\n            True\n            &gt;&gt;&gt; args[0] == \"litestream\"\n            True\n            &gt;&gt;&gt; args[1] == \"restore\"\n            True\n            &gt;&gt;&gt; args[-2] == str(stream.dbpath)\n            True\n            &gt;&gt;&gt; args[-1] == stream.s3\n            True\n        \"\"\"  # noqa: E501\nreturn [\n\"litestream\",\n\"restore\",\n\"-v\",  # verbose\n\"-o\",  # output\nf\"{str(self.dbpath)}\",  # output path\nself.s3,  # source of restore\n]\ndef restore(self) -&gt; Path:\n\"\"\"Runs the pre-configured litestream command (`@restore_args`) to restore the\n        database from the replica url to the constructed database path at `@dbpath`.\n        No need to use a timeout here since after restoration, the command terminates.\n        This is unlike `self.timed_replicate()` which is continuously executed even\n        after replication.\n        \"\"\"\ncmd = {\" \".join(self.restore_args)}\nlogger.info(f\"Run: {cmd}\")\nproc: CompletedProcess = subprocess.run(\nself.restore_args, capture_output=True\n)\nfor line in proc.stderr.splitlines():\nlogger.debug(line)\nreturn self.dbpath\ndef delete(self):\n\"\"\"Deletes the file located at the constructed database path `@dbpath`.\n        Examples:\n            &gt;&gt;&gt; from pylts import ConfigS3\n            &gt;&gt;&gt; from pathlib import Path\n            &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n            &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n            &gt;&gt;&gt; stream.dbpath.exists()\n            True\n            &gt;&gt;&gt; stream.delete()\n            &gt;&gt;&gt; stream.dbpath.exists()\n            False\n        \"\"\"  # noqa: E501\nlogger.warning(f\"Deleting {self.dbpath=}\")\nself.dbpath.unlink(missing_ok=True)\ndef get_result_on_timeout(\nself, cmd: list[str], timeout: int\n) -&gt; tuple[str, str]:\n\"\"\"Returns results of a long-running process defined by `cmd` after the\n        expiration of the `timeout`. This is an adoption of the python sample [code](https://docs.python.org/3.11/library/subprocess.html#subprocess.Popen.communicate)\n        re: `Popen.communicate()`\n        Because of the expiration of the timeout is an error, it falls into the second half of the\n        tuple of strings returned.\n        \"\"\"  # noqa: E501\np = Popen(cmd, text=True, stdout=PIPE, stderr=PIPE)\ntry:\nlogger.info(f\"Output: {cmd=}\")\nreturn p.communicate(timeout=timeout)\nexcept TimeoutExpired:\nlogger.info(f\"Timed Out: {cmd=}\")\np.kill()\nreturn p.communicate()\ndef timed_replicate(self, timeout_seconds: int) -&gt; bool:\n\"\"\"Replication from litestream is a continuous, non-terminating command,\n        hence the need for `timed_replicate()` to ensure that we only\n        replicate a single time.\n        We enforce this rule by ensuring that the replication process performed\n        by `@replicate_args` should becompleted within `timeout_seconds`. Whether or not\n        a replication is done will determine the acts to be performed and the value of\n        the resulting boolean.\n        If replication is successful, a snapshot is written to the `s3` url and the\n        local `@dbpath` can be (and is) deleted.\n        Args:\n            timeout_seconds (int): Number of seconds\n        Returns:\n            bool: Whether the replication was successful within `timeout_seconds`\n        \"\"\"\nlogger.info(\nf\"Replication to {self.s3=} start: {datetime.datetime.now()=}\"\n)\nres = self.get_result_on_timeout(self.replicate_args, timeout_seconds)\n_, stderr_data = res[0], res[1]  # stderr because of timeout err\nfor text in stderr_data.splitlines():\nif \"snapshot written\" in text:  # see litestream prompt\nlogger.success(f\"Snapshot on {datetime.datetime.now()=}\")\nself.dbpath.unlink()  # delete the file after replication\nreturn True\nreturn False\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3-attributes","title":"Attributes","text":""},{"location":"config/#pylts.aws.ConfigS3.dbpath","title":"<code>dbpath: Path</code>  <code>property</code>","text":"<p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; stream.dbpath == Path().cwd() / \"data\" / \"db.sqlite\" # Automatic construction of default db name\nTrue\n</code></pre> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Where the database will be located locally.</p>"},{"location":"config/#pylts.aws.ConfigS3.replicate_args","title":"<code>replicate_args: list[str]</code>  <code>property</code>","text":"<p>When used in the command line <code>litestream replicate &lt;dbpath&gt; &lt;replica_url&gt;</code> works. As a subprocess, we itemize each item for future use.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; args = stream.replicate_args\n&gt;&gt;&gt; isinstance(stream.replicate_args, list)\nTrue\n&gt;&gt;&gt; args[0] == \"litestream\"\nTrue\n&gt;&gt;&gt; args[1] == \"replicate\"\nTrue\n&gt;&gt;&gt; args[2] == str(stream.dbpath)\nTrue\n&gt;&gt;&gt; args[3] == stream.s3\nTrue\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3.restore_args","title":"<code>restore_args: list[str]</code>  <code>property</code>","text":"<p>When used in the command line <code>litestream restore -o &lt;dbpath&gt; &lt;replica_url&gt;</code> works. As a subprocess, we itemize each item for future use.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; args = stream.restore_args\n&gt;&gt;&gt; isinstance(stream.restore_args, list)\nTrue\n&gt;&gt;&gt; args[0] == \"litestream\"\nTrue\n&gt;&gt;&gt; args[1] == \"restore\"\nTrue\n&gt;&gt;&gt; args[-2] == str(stream.dbpath)\nTrue\n&gt;&gt;&gt; args[-1] == stream.s3\nTrue\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3-functions","title":"Functions","text":""},{"location":"config/#pylts.aws.ConfigS3.delete","title":"<code>delete()</code>","text":"<p>Deletes the file located at the constructed database path <code>@dbpath</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from pylts import ConfigS3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n&gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n&gt;&gt;&gt; stream.dbpath.exists()\nTrue\n&gt;&gt;&gt; stream.delete()\n&gt;&gt;&gt; stream.dbpath.exists()\nFalse\n</code></pre> Source code in <code>pylts/aws.py</code> Python<pre><code>def delete(self):\n\"\"\"Deletes the file located at the constructed database path `@dbpath`.\n    Examples:\n        &gt;&gt;&gt; from pylts import ConfigS3\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; # The key, token, s3 are usually just set up in an .env file. They're included here for testing purposes. The folder however is advised to be explicitly declared\n        &gt;&gt;&gt; stream = ConfigS3(key=\"xxx\", token=\"yyy\", s3=\"s3://x/x.db\", folder=Path().cwd() / \"data\")\n        &gt;&gt;&gt; stream.dbpath.exists()\n        True\n        &gt;&gt;&gt; stream.delete()\n        &gt;&gt;&gt; stream.dbpath.exists()\n        False\n    \"\"\"  # noqa: E501\nlogger.warning(f\"Deleting {self.dbpath=}\")\nself.dbpath.unlink(missing_ok=True)\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3.get_result_on_timeout","title":"<code>get_result_on_timeout(cmd, timeout)</code>","text":"<p>Returns results of a long-running process defined by <code>cmd</code> after the expiration of the <code>timeout</code>. This is an adoption of the python sample code re: <code>Popen.communicate()</code></p> <p>Because of the expiration of the timeout is an error, it falls into the second half of the tuple of strings returned.</p> Source code in <code>pylts/aws.py</code> Python<pre><code>def get_result_on_timeout(\nself, cmd: list[str], timeout: int\n) -&gt; tuple[str, str]:\n\"\"\"Returns results of a long-running process defined by `cmd` after the\n    expiration of the `timeout`. This is an adoption of the python sample [code](https://docs.python.org/3.11/library/subprocess.html#subprocess.Popen.communicate)\n    re: `Popen.communicate()`\n    Because of the expiration of the timeout is an error, it falls into the second half of the\n    tuple of strings returned.\n    \"\"\"  # noqa: E501\np = Popen(cmd, text=True, stdout=PIPE, stderr=PIPE)\ntry:\nlogger.info(f\"Output: {cmd=}\")\nreturn p.communicate(timeout=timeout)\nexcept TimeoutExpired:\nlogger.info(f\"Timed Out: {cmd=}\")\np.kill()\nreturn p.communicate()\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3.restore","title":"<code>restore()</code>","text":"<p>Runs the pre-configured litestream command (<code>@restore_args</code>) to restore the database from the replica url to the constructed database path at <code>@dbpath</code>. No need to use a timeout here since after restoration, the command terminates. This is unlike <code>self.timed_replicate()</code> which is continuously executed even after replication.</p> Source code in <code>pylts/aws.py</code> Python<pre><code>def restore(self) -&gt; Path:\n\"\"\"Runs the pre-configured litestream command (`@restore_args`) to restore the\n    database from the replica url to the constructed database path at `@dbpath`.\n    No need to use a timeout here since after restoration, the command terminates.\n    This is unlike `self.timed_replicate()` which is continuously executed even\n    after replication.\n    \"\"\"\ncmd = {\" \".join(self.restore_args)}\nlogger.info(f\"Run: {cmd}\")\nproc: CompletedProcess = subprocess.run(\nself.restore_args, capture_output=True\n)\nfor line in proc.stderr.splitlines():\nlogger.debug(line)\nreturn self.dbpath\n</code></pre>"},{"location":"config/#pylts.aws.ConfigS3.timed_replicate","title":"<code>timed_replicate(timeout_seconds)</code>","text":"<p>Replication from litestream is a continuous, non-terminating command, hence the need for <code>timed_replicate()</code> to ensure that we only replicate a single time.</p> <p>We enforce this rule by ensuring that the replication process performed by <code>@replicate_args</code> should becompleted within <code>timeout_seconds</code>. Whether or not a replication is done will determine the acts to be performed and the value of the resulting boolean.</p> <p>If replication is successful, a snapshot is written to the <code>s3</code> url and the local <code>@dbpath</code> can be (and is) deleted.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_seconds</code> <code>int</code> <p>Number of seconds</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the replication was successful within <code>timeout_seconds</code></p> Source code in <code>pylts/aws.py</code> Python<pre><code>def timed_replicate(self, timeout_seconds: int) -&gt; bool:\n\"\"\"Replication from litestream is a continuous, non-terminating command,\n    hence the need for `timed_replicate()` to ensure that we only\n    replicate a single time.\n    We enforce this rule by ensuring that the replication process performed\n    by `@replicate_args` should becompleted within `timeout_seconds`. Whether or not\n    a replication is done will determine the acts to be performed and the value of\n    the resulting boolean.\n    If replication is successful, a snapshot is written to the `s3` url and the\n    local `@dbpath` can be (and is) deleted.\n    Args:\n        timeout_seconds (int): Number of seconds\n    Returns:\n        bool: Whether the replication was successful within `timeout_seconds`\n    \"\"\"\nlogger.info(\nf\"Replication to {self.s3=} start: {datetime.datetime.now()=}\"\n)\nres = self.get_result_on_timeout(self.replicate_args, timeout_seconds)\n_, stderr_data = res[0], res[1]  # stderr because of timeout err\nfor text in stderr_data.splitlines():\nif \"snapshot written\" in text:  # see litestream prompt\nlogger.success(f\"Snapshot on {datetime.datetime.now()=}\")\nself.dbpath.unlink()  # delete the file after replication\nreturn True\nreturn False\n</code></pre>"},{"location":"docker/","title":"Dockerfile Sample","text":"<p>The steps included in the Dockerfile can be used as base example:</p> Elements Version Version List python <code>3.11.1</code> versions litestream <code>0.3.9</code> versions sqlite <code>3.40.1</code> versions"},{"location":"docker/#base","title":"Base","text":"<p>Get preliminary tools to process litestream and sqlite</p> Docker<pre><code>RUN apt update &amp;&amp; apt install -y build-essential wget pkg-config git &amp;&amp; apt clean\n</code></pre>"},{"location":"docker/#litestream","title":"Litestream","text":"Docker<pre><code># As of Feb 2023: latest version\nARG LITESTREAM_VER=0.3.9\nADD https://github.com/benbjohnson/litestream/releases/download/v$LITESTREAM_VER/litestream-v$LITESTREAM_VER-linux-amd64-static.tar.gz /tmp/litestream.tar.gz\nRUN tar -C /usr/local/bin -xzf /tmp/litestream.tar.gz\n</code></pre> <p>Review the latest version to get the updated release from github.</p>"},{"location":"docker/#sqlite","title":"sqlite","text":"<p>The version that comes with python isn't the most updated sqlite version, hence need to compile the latest one and configure with extensions:</p> Docker<pre><code># As of Feb 2023: latest version\nARG SQLITE_YEAR=2022\nARG SQLITE_VER=3400100\nRUN wget \"https://www.sqlite.org/$SQLITE_YEAR/sqlite-autoconf-$SQLITE_VER.tar.gz\" \\\n&amp;&amp; tar xzf sqlite-autoconf-$SQLITE_VER.tar.gz \\\n&amp;&amp; cd sqlite-autoconf-$SQLITE_VER \\\n&amp;&amp; ./configure --disable-static --enable-fts5 --enable-json1 CFLAGS=\"-g -O2 -DSQLITE_ENABLE_JSON1\" \\\n&amp;&amp; make &amp;&amp; make install\n</code></pre>"},{"location":"docker/#build-run-sample","title":"Build / Run Sample","text":"Bash<pre><code>export LITESTREAM_ACCESS_KEY_ID=xxx\nexport LITESTREAM_SECRET_ACCESS_KEY=yyy\nexport REPLICA_URL=s3://x/x.db\npoetry export -f requirements.txt -o requirements.txt --without-hashes\ndocker build .\ndocker run it \\\n-e LITESTREAM_ACCESS_KEY_ID \\\n-e LITESTREAM_SECRET_ACCESS_KEY \\\n-e REPLICA_URL\n</code></pre>"}]}